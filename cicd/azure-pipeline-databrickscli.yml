trigger:
  branches:
    include:
      - develop
      - main
  paths:
    include:
      - '$(SOURCE_FILES_PATH)/*'

pool:
  vmImage: ubuntu-latest

stages:
  - stage: CI
    displayName: "Continuous Integration"
    jobs:
      - job: CIJob
        displayName: "Countinuous Integration Steps"
        steps:
          - checkout: self
            clean: true
          - task: UsePythonVersion@0
            displayName: 'Use Python 3.9'
            inputs:
              versionSpec: 3.9

          - script: |
              pip install pytest
            displayName: 'Load Python dependencies'
            
          - script: |
              sudo apt-get update
              sudo apt-get install -y jq
            displayName: 'Instalar jq'

  - stage: CD
    displayName: "Continuous Deployment"      
    dependsOn: CI
    jobs:
    #- job: DeployToDev
    #  displayName: "Deploy to Dev Environment"
    #  steps:
    #    - script: |
    #        curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
    #      displayName: 'download and install databricks-cli'

    #    - script: |
    #        echo $(DEV_BEARER_TOKEN) | databricks configure --host $(DEV_DATABRICKS_URL) --token $(DEV_BEARER_TOKEN) 
    #      displayName: 'Configure DataBricks DEV CLI'

    #    - script: |
    #        databricks workspace mkdirs "/$(TARGET_FILES_PATH)/"
    #      displayName: 'Create App Dir if it does not exist'

    #    - script: |
    #        databricks workspace delete "/$(TARGET_FILES_PATH)/" --recursive
    #      displayName: 'Clean Workspace'

    #    - script: |
    #        databricks workspace import-dir --overwrite ./$(SOURCE_FILES_PATH)/ "/$(TARGET_FILES_PATH)/"
    #      displayName: 'Copy to Workspace'

    - job: DeployToProd
      displayName: "Deploy to Prod Environment"
    #  dependsOn: "DeployToDev"
      steps:
        - script: |
            curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          displayName: 'download and install databricks-cli'

        - script: |
            echo $(PRO_BEARER_TOKEN) | databricks configure --host $(PRO_DATABRICKS_URL) --token $(PRO_BEARER_TOKEN) 
          displayName: 'Configure DataBricks DEV CLI'

      #  - script: |
      #      databricks workspace mkdirs "/$(TARGET_FILES_PATH)/"
      #    displayName: 'Create App Dir if it does not exist'

      #  - script: |
      #      databricks workspace delete "/$(TARGET_FILES_PATH)/" --recursive
      #    displayName: 'Clean Workspace'

      #  - script: |
      #      databricks workspace import-dir --overwrite ./$(SOURCE_FILES_PATH)/ "/$(TARGET_FILES_PATH)/"
      #    displayName: 'Copy to Workspace'
        
        - script: |
            jq '.tasks[].existing_cluster_id = "$CLUSTER_ID"' $(DATABRICKS_JOB_FILE) > updated-job-config.json 
            mv updated-job-config.json $(DATABRICKS_JOB_FILE)
          displayName: 'Update existing_cluster_id in Json file'

        - script: |
            JOB_ID=$(databricks jobs list | grep $(DATABRICKS_JOB_NAME) | awk '{print $1}')
            echo "O ID do Job é: $JOB_ID"
            ls -la $(Build.SourcesDirectory)
            echo "Arquivos no diretório cicd (artefatos):"
            ls -la $(Build.SourcesDirectory)/cicd
            if [ -z "$JOB_ID" ]; then
              echo "Job não encontrado, criando novo..."
              #echo "Usando o arquivo JSON em: $(DATABRICKS_JOB_FILE)"
              databricks jobs create --json-file $(Build.SourcesDirectory)/cicd/job-config.json --debug
            else
              echo "Job encontrado, atualizando..."
              #echo "Usando o arquivo JSON em: $JSON_FILE_PATH"
              databricks jobs reset --job-id 218158619014638 --json-file cicd/job-config.json --debug
            fi
          displayName: 'Create or Update Job in Databricks'